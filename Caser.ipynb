{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class Caser(nn.Module):\n",
    "    def __init__(self,num_factors, num_users, num_items, L=5, d=16,\n",
    "                 d_prime=4, drop_ratio=0.05):\n",
    "        \"\"\"\n",
    "        :param num_factors: 嵌入向量长度\n",
    "        :param num_users:\n",
    "        :param num_items:\n",
    "        :param L:\n",
    "        :param d:\n",
    "        :param d_prime:\n",
    "        :param drop_ratio:\n",
    "        \"\"\"\n",
    "        super(Caser, self).__init__()\n",
    "        self.P = nn.Embedding(num_users,num_factors)\n",
    "        self.Q = nn.Embedding(num_items,num_factors)\n",
    "        self.d_prime, self.d = d_prime, d\n",
    "        # Vertical convolution layer\n",
    "        self.conv_v = nn.Conv2d(in_channels=1,out_channels=d_prime,kernel_size=(L, 1))\n",
    "        # Horizontal convolution layer\n",
    "        h = [i + 1 for i in range(L)]\n",
    "        self.conv_h, self.max_pool = nn.Sequential(), nn.Sequential()\n",
    "        for i in h:\n",
    "            self.conv_h.add_module('conv_h'+str(i),nn.Conv2d(in_channels=1,out_channels=d,kernel_size=(i, num_factors)))\n",
    "            self.max_pool.add_module('max_pool'+str(i),nn.MaxPool1d(L - i + 1))\n",
    "        # Fully-connected layer\n",
    "        self.fc1_dim_v, self.fc1_dim_h = d_prime * num_factors, d * len(h)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=d_prime * num_factors + d * L,out_features=num_factors),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.V = nn.Embedding(num_items, num_factors * 2)\n",
    "        self.b = nn.Embedding(num_items, 1)\n",
    "        self.dropout = nn.Dropout(drop_ratio)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, user_id, seq, item_id):\n",
    "        item_embs = torch.unsqueeze(self.Q(seq), 1) # (batch_size,1,L,num_factors)\n",
    "        user_emb = self.P(user_id) # (batch_size,num_factors)\n",
    "        out, out_h, out_v, out_hs = None, None, None, []\n",
    "        if self.d_prime:\n",
    "            out_v = self.conv_v(item_embs) # (batch_size,d_prime,1,num_factors)\n",
    "            out_v = out_v.reshape(out_v.shape[0], self.fc1_dim_v) # (batch_size,d_prime*num_factors)\n",
    "        if self.d:\n",
    "            for conv, maxp in zip(self.conv_h, self.max_pool):\n",
    "                conv_out = torch.squeeze(self.relu(conv(item_embs)), dim=3) # (batch_size,1,L,num_factors) -conv> (batch_size,d,L->1,1) -squeeze> (batch_size,d,L->1)\n",
    "                t = maxp(conv_out) # (batch_size,d,1)\n",
    "                pool_out = torch.squeeze(t, dim=2) # (batch_size,d)\n",
    "                out_hs.append(pool_out)\n",
    "            out_h = torch.concat(out_hs, dim=1) # (batch_size,L*d)\n",
    "        out = torch.concat([out_v, out_h], dim=1) # (batch_size,L*d+d_prime*num_factors)\n",
    "        z = self.fc(self.dropout(out)) # (batch_size,num_factors)\n",
    "        x = torch.concat([z, user_emb], dim=1) # (batch_size,2*num_factors)\n",
    "        q_prime_i = torch.squeeze(self.V(item_id)) # (batch_size,2*num_factors)\n",
    "        b = torch.squeeze(self.b(item_id)) # (batch_size,1)\n",
    "        res = (x * q_prime_i).sum(1) + b # (batch_size,2*num_factors) -sum> (batch_size,1)+(batch_size,1)\n",
    "        return res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, user_ids, item_ids, L, num_users, num_items,candidates):\n",
    "        \"\"\"\n",
    "        :param user_ids:数据集中按user_id和时间排序后的user_id，范围[0,num_users-1]\n",
    "        :param item_ids:数据集中对应user_ids顺序的item_id，范围[0,num_items-1]\n",
    "        :param L:输出前L个交互项目作为一个序列，下一个项目作为目标\n",
    "        :param num_users:\n",
    "        :param num_items:\n",
    "        :param candidates: 字典 key为user，value为按时间排序的item list\n",
    "        \"\"\"\n",
    "        user_ids, item_ids = np.array(user_ids), np.array(item_ids)\n",
    "        temp, self.cand = {}, candidates\n",
    "        self.all_items = set([i for i in range(num_items)]) #set [0,num_items-1]\n",
    "        [temp.setdefault(user_ids[i], []).append(i) for i, _ in enumerate(user_ids)] #字典，user_id作为key，value为相同user_id重复出现的索引列表，和item_ids一致\n",
    "        temp = sorted(temp.items(), key=lambda x: x[0]) #变成list，并按value的第一个索引排序\n",
    "        u_ids = np.array([i[0] for i in temp])  #所有temp中的user_id\n",
    "        idx = np.array([i[1][0] for i in temp]) #不同user_id交互的开始的索引\n",
    "        self.ns = int(sum([c - L if c >= L + 1 else 1 for c  # 计算每个user_id有多少窗口数，求和也就是SeqDataset能提供的训练数据的数量\n",
    "                                in np.array([len(i[1]) for i in temp])]))\n",
    "        self.seq_items = np.zeros((self.ns, L))  #长度不超过L的训练item X\n",
    "        self.seq_users = np.zeros(self.ns, dtype='int32')\n",
    "        self.seq_tgt = np.zeros((self.ns, 1)) #预测目标 Y\n",
    "        self.test_seq = np.zeros((num_users, L)) #测试集 X\n",
    "        test_users, _uid = np.empty(num_users), None\n",
    "        for i, (uid, i_seq) in enumerate(self._seq(u_ids, item_ids, idx, L + 1)):\n",
    "            if uid != _uid:\n",
    "                self.test_seq[uid][:] = i_seq[-L:] #第一个产生的是最新的序列，i_seq最后L个作为test_seq[uid]，是测试集的输入X\n",
    "                test_users[uid], _uid = uid, uid\n",
    "            self.seq_tgt[i][:] = i_seq[-1:] #i_seq最后1个作为预测目标\n",
    "            self.seq_items[i][:], self.seq_users[i] = i_seq[:L], uid #前L个作为输入X\n",
    "\n",
    "    def _win(self, tensor, window_size, step_size=1): #生成窗口大小最大为window_size的序列\n",
    "        if len(tensor) - window_size >= 0:\n",
    "            for i in range(len(tensor), 0, - step_size): #从后到前\n",
    "                if i - window_size >= 0:\n",
    "                    yield tensor[i - window_size:i]\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            yield tensor\n",
    "\n",
    "    def _seq(self, u_ids, item_ids, idx, max_len): #生成器，每调用一次生成一个训练数据\n",
    "        for i in range(len(idx)): #对所有唯一的user_id\n",
    "            stop_idx = None if i >= len(idx) - 1 else int(idx[i + 1]) # 本user_id的最后一个索引是下一个user_id第一个索引的前一个\n",
    "            for s in self._win(item_ids[int(idx[i]):stop_idx], max_len):\n",
    "                yield (int(u_ids[i]), s)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ns\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        :return: 第i个训练数据的uid,第i个训练数据的序列,第i个训练数据的目标,负例\n",
    "        \"\"\"\n",
    "        neg = list(self.all_items - set(self.cand[int(self.seq_users[idx])])) #负例\n",
    "        i = random.randint(0, len(neg) - 1)\n",
    "        return (self.seq_users[idx], self.seq_items[idx], self.seq_tgt[idx],\n",
    "                neg[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class ArrayDataset(Dataset):\n",
    "    def __init__(self, ArrayData,seq):\n",
    "        self._data  = ArrayData\n",
    "        self.seq = seq\n",
    "    def __len__(self):\n",
    "        return len(self._data[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._data[0][idx],self._data[1][idx],self.seq"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def hit_and_auc(rankedlist, test_matrix, k):\n",
    "    \"\"\"\n",
    "    计算每个用户的命中数和 AUC。\n",
    "    :param rankedlist:\n",
    "    :param test_matrix: 用户真正交互的item\n",
    "    :param k:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    hits_k = [(idx, val) for idx, val in enumerate(rankedlist[:k])\n",
    "              if val in set(test_matrix)]\n",
    "    hits_all = [(idx, val) for idx, val in enumerate(rankedlist)\n",
    "                if val in set(test_matrix)]\n",
    "    max = len(rankedlist) - 1\n",
    "    auc = 1.0 * (max - hits_all[0][0]) / max if len(hits_all) > 0 else 0\n",
    "    return len(hits_k), auc\n",
    "\n",
    "\n",
    "def evaluate_ranking(net, test_input, candidates,num_users, num_items,device):\n",
    "    ranked_list, ranked_items, hit_rate, auc = {}, {}, [], []\n",
    "    all_items = set([i for i in range(num_items)])\n",
    "    for u in tqdm(range(num_users)):\n",
    "        u_seq = test_input[0][u]\n",
    "        true_item = test_input[1][u]\n",
    "        neg_items = list(all_items - set(candidates[int(u)]))\n",
    "        user_ids, item_ids, x, scores = [], [], [], []\n",
    "        [item_ids.append(i) for i in neg_items] #记录u没有评价的item id\n",
    "        [user_ids.append(u) for _ in neg_items] #u的id，和item_ids长度相同\n",
    "        x.extend([np.array(user_ids)])\n",
    "        x.extend([np.array(item_ids)])\n",
    "        # x[0]:len=len(neg_items) 元素全为u,x[1]:len=len(neg_items) 元素为neg_items的id\n",
    "        x = np.array(x)\n",
    "        x = torch.tensor(x)\n",
    "        test_data_set = ArrayDataset(x,u_seq)\n",
    "        test_data_iter = DataLoader(test_data_set, shuffle=False, batch_size=1024)\n",
    "        for user_id, item_id,seq in test_data_iter:\n",
    "            user_id = torch.as_tensor(user_id,dtype=torch.int,device=device)\n",
    "            item_id = torch.as_tensor(item_id,dtype=torch.int,device=device)\n",
    "            seq = torch.as_tensor(seq,dtype=torch.int,device=device)\n",
    "            score = net(user_id,seq,item_id)\n",
    "            scores.extend(score)\n",
    "        # print(scores)\n",
    "        # scores = [item for sublist in scores for item in sublist]\n",
    "        item_scores = list(zip(item_ids, scores))\n",
    "        ranked_list[u] = sorted(item_scores, key=lambda t: t[1], reverse=True)\n",
    "        ranked_items[u] = [r[0] for r in ranked_list[u]]\n",
    "\n",
    "        temp = hit_and_auc(ranked_items[u], true_item, 50)\n",
    "        hit_rate.append(temp[0])\n",
    "        auc.append(temp[1])\n",
    "    return np.mean(np.array(hit_rate)), np.mean(np.array(auc))\n",
    "\n",
    "def train_ranking(net, train_iter, test_input, optimizer,loss,num_users,num_items, num_epochs, device, evaluator,candidates):\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    plt_epoch = []\n",
    "    for epoch in range(num_epochs):\n",
    "        plt_epoch.append(epoch)\n",
    "        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()\n",
    "        for u,seq,pos_item,neg_item in tqdm(train_iter):\n",
    "            seq = torch.as_tensor(seq,dtype=torch.int,device=device)\n",
    "            u = torch.as_tensor(u,dtype=torch.int,device=device)\n",
    "            pos_item = torch.as_tensor(pos_item,dtype=torch.int,device=device)\n",
    "            neg_item = torch.as_tensor(neg_item,dtype=torch.int,device=device)\n",
    "\n",
    "            p_pos = net(u,seq,pos_item)\n",
    "            p_neg = net(u,seq,neg_item)\n",
    "            l = loss(p_pos, p_neg)\n",
    "            train_l_sum += l.sum().cpu().item()\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            batch_count+=1\n",
    "        with torch.no_grad():\n",
    "            hit_rate, auc = evaluator(net, test_input,candidates,num_users, num_items,device)\n",
    "        print('epoch %d,train_loss %.4f,hit_rate %.4f,auc %.4f, time %.1f sec'\n",
    "              % (epoch + 1,train_l_sum / batch_count,hit_rate,auc, time.time() - start))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_data_ml100k():\n",
    "    \"\"\"\n",
    "    从u.data读取数据转成dataFrame\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "    data = pd.read_csv('../../data/ml-100k/u.data', '\\t', names=names,engine='python')\n",
    "    num_users = data.user_id.unique().shape[0]\n",
    "    num_items = data.item_id.unique().shape[0]\n",
    "    return data, num_users, num_items\n",
    "\n",
    "def split_data_ml100k(data, num_users, split_mode='random', test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    将数据集按随机模式或时序模式分割成训练集和测试集\n",
    "    :param data:\n",
    "    :param num_users:\n",
    "    :param split_mode:\n",
    "    :param test_ratio:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if split_mode == 'seq-aware':\n",
    "        train_items, test_items, train_list = {}, {}, []\n",
    "        for line in data.itertuples():\n",
    "            u, i, rating, time = line[1], line[2], line[3], line[4]\n",
    "            train_items.setdefault(u, []).append((u, i, rating, time)) # 如果键不在字典里，setdefault将键和默认值添加到字典中，最后返回该键对应的值。这里返回list，通过append将所有相同user的item放到同一个key对应的value里。\n",
    "            if u not in test_items or test_items[u][-1] < time: # 将最新的item放到test_items\n",
    "                test_items[u] = (i, rating, time)\n",
    "        for u in range(1, num_users + 1): #导致train_data按u从小到大排序\n",
    "            train_list.extend(sorted(train_items[u], key=lambda k: k[3])) # 将每个user对应的value 按照时间戳从小到大排序放到train_list\n",
    "        test_data = [(key, *value) for key, value in test_items.items()] # 将test_items变成list\n",
    "        train_data = [item for item in train_list if item not in test_data] #将train_list不在test_data里的元素放到train_data\n",
    "        train_data = pd.DataFrame(train_data,columns=['u', 'i', 'rating', 'time'])\n",
    "        test_data = pd.DataFrame(test_data,columns=['u', 'i', 'rating', 'time']).sort_values(by='u')\n",
    "    else:\n",
    "        mask = np.random.uniform(0, 1, (len(data))) < (1 - test_ratio)# 生成(len(data),)大小的bool类型数组 随机test_ratio比例的元素为False，其余为True\n",
    "        neg_mask = [not x for x in mask] # 生成len(data)长度的bool类型list 元素和mask相反\n",
    "        train_data, test_data = data[mask], data[neg_mask]\n",
    "    return train_data, test_data\n",
    "\n",
    "def load_data_ml100k(data, num_users, num_items, feedback='explicit'):\n",
    "    \"\"\"\n",
    "    从训练集或测试集中分出用户列表、item列表、得分列表，给出显式或隐式交互矩阵\n",
    "    :param data:\n",
    "    :param num_users:\n",
    "    :param num_items:\n",
    "    :param feedback:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    users, items, scores = [], [], []\n",
    "    inter = np.zeros((num_items, num_users)) if feedback == 'explicit' else {}\n",
    "    for line in data.itertuples():\n",
    "        user_index, item_index = int(line[1] - 1), int(line[2] - 1) #0~942 0~1681\n",
    "        score = int(line[3]) if feedback == 'explicit' else 1\n",
    "        users.append(user_index)\n",
    "        items.append(item_index)\n",
    "        scores.append(score)\n",
    "        if feedback == 'implicit':\n",
    "            inter.setdefault(user_index, []).append(item_index) # 隐式则为字典 key为user，value为按时间排序的item list\n",
    "        else:\n",
    "            inter[item_index, user_index] = score\n",
    "    return users, items, scores, inter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12548\\AppData\\Local\\Temp/ipykernel_6552/1025409491.py:2: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only\n",
      "  df, num_users, num_items = d2l.read_data_ml100k()\n"
     ]
    }
   ],
   "source": [
    "TARGET_NUM, L, batch_size = 1, 5, 2048\n",
    "df, num_users, num_items = read_data_ml100k()\n",
    "train_data, test_data = split_data_ml100k(df, num_users, 'seq-aware')\n",
    "users_train, items_train, ratings_train, candidates = load_data_ml100k(\n",
    "    train_data, num_users, num_items, feedback=\"implicit\")\n",
    "_, _, _, test_seq_Y = load_data_ml100k(\n",
    "    test_data, num_users, num_items, feedback=\"implicit\")\n",
    "\n",
    "train_set = SeqDataset(users_train, items_train, L, num_users,\n",
    "                            num_items, candidates)\n",
    "train_iter = DataLoader(train_set, batch_size, True)\n",
    "test_seq_X = train_set.test_seq\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = Caser(10, num_users, num_items, L)\n",
    "num_epochs, wd, optimizer = 3, 1e-5, torch.optim.Adam(net.parameters(), lr=0.04)\n",
    "def BPRLoss(positive,negative):\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    return - torch.sum(torch.log(sigmoid(positive - negative)), dim=0, keepdim=True)\n",
    "loss = BPRLoss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:06<00:00,  7.37it/s]\n",
      "100%|██████████| 943/943 [19:09<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1,train_loss 2556.7216,hit_rate 0.0604,auc 0.6387, time 1156.3 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:07<00:00,  6.11it/s]\n",
      "100%|██████████| 943/943 [19:55<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2,train_loss 1001.9722,hit_rate 0.1241,auc 0.7295, time 1203.6 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:06<00:00,  7.35it/s]\n",
      "100%|██████████| 943/943 [21:07<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3,train_loss 718.7881,hit_rate 0.1569,auc 0.7626, time 1274.4 sec\n"
     ]
    }
   ],
   "source": [
    "train_ranking(net, train_iter, [test_seq_X,test_seq_Y], optimizer,loss,num_users,num_items, num_epochs, device, evaluate_ranking,candidates)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-ff83aeb4",
   "language": "python",
   "display_name": "PyCharm (MLWorkspace)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}